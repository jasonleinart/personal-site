---
title: "Contract Review Demo"
role: "Builder"
year: 2026
outcomeSummary: "Working RAG-based contract analysis tool demonstrating legal AI implementation principles"
overview: "A demonstration implementation of the architecture decisions outlined in the Legal AI Roadmap—proving that 'courtroom-grade' reliability is achievable with the right configuration choices."
problem: "General-purpose LLMs hallucinate in 49% of legal tasks. The Legal AI Roadmap outlines the strategic framework, but strategy without implementation is just theory. This demo bridges that gap."
constraints:
  - "Must demonstrate RAG architecture, not just prompt engineering"
  - "Must include citation verification (not just retrieval)"
  - "Must measure reliability with quantifiable metrics"
  - "Must be explainable—show why decisions were made"
  - "Cannot use proprietary legal databases (Westlaw, Lexis) for demo"
approach: "Build a contract analysis tool that implements legal-specific RAG configuration, then document every architectural decision with rationale. The goal isn't a production system—it's a proof of concept that demonstrates understanding of the implementation challenges."
keyDecisions:
  - decision: "Legal-aware chunking over standard 400-token chunks"
    reasoning: "Standard chunking breaks legal text in dangerous ways. A split mid-clause can invert meaning. Contract sections must stay intact with their defined terms."
    alternatives:
      - "Standard 400-token chunks with overlap"
      - "Sentence-level chunking"
      - "Paragraph-level chunking"
  - decision: "Hybrid search (vector + keyword) over pure vector search"
    reasoning: "'Shall' and 'may' embed to nearly identical vectors but have opposite legal implications. Statutory citations like '42 U.S.C. § 1983' need exact matching, not semantic approximation."
    alternatives:
      - "Pure vector similarity search"
      - "Pure keyword (BM25) search"
  - decision: "Explicit refusal behavior over always-answer approach"
    reasoning: "A system that always attempts an answer is dangerous for legal work. When retrieval confidence is low, returning 'I cannot provide a reliable answer' is better than guessing."
    alternatives:
      - "Always provide best-effort answer with confidence score"
      - "Return top results without synthesis"
  - decision: "Open-source evaluation metrics over proprietary benchmarks"
    reasoning: "Reproducibility matters. Using RAGAS for faithfulness and open datasets allows others to verify claims and adapt the approach."
    alternatives:
      - "Custom internal benchmarks"
      - "Proprietary evaluation services"
techStack:
  - "Python"
  - "LlamaIndex"
  - "Qdrant"
  - "Claude API"
  - "RAGAS"
  - "Streamlit"
impact:
  metrics:
    - label: "Faithfulness Score"
      value: "Target >0.95 (legal threshold)"
    - label: "Citation Accuracy"
      value: "Target 100% (no fabricated citations)"
    - label: "Refusal Rate"
      value: "Target >5% (system acknowledges uncertainty)"
  qualitative: "Demonstrates that the principles in the Legal AI Roadmap translate to working code. Provides a reference implementation for legal-specific RAG configuration."
learnings:
  - "Chunking strategy has more impact on legal RAG quality than model selection"
  - "Hybrid search is mandatory for legal text—pure vector search fails on precision terms"
  - "Evaluation metrics must be built into the pipeline, not added after"
  - "Refusal behavior is a feature, not a bug—systems that always answer are dangerous"
featured: true
status: ongoing
order: 1
---

## Why This Demo Exists

The [Legal AI Roadmap](/analysis/legal-ai-roadmap) provides the strategic framework for law firm AI adoption. But I've sat in enough meetings where impressive-sounding strategy slides never became working systems.

This demo exists to prove two things:
1. The architectural recommendations in that roadmap are implementable
2. I understand the implementation challenges, not just the strategy

## What "Courtroom-Grade" Actually Requires

The phrase sounds good in presentations. Here's what it means technically:

**Citation accuracy must be verifiable.** Not "based on retrieved context" but exact source, section, and clause. Every claim links to a specific document location.

**Hallucination rate below 1%.** Standard RAG tolerates 5-10% for general applications. Legal work requires under 1%, with zero tolerance for fabricated citations.

**Refusal behavior for uncertainty.** The system must refuse when retrieval confidence is low. "I cannot provide a reliable answer" beats a confident-sounding guess.

**Temporal awareness.** Contracts have effective dates. Amendments supersede original language. The system must track document versions.

## Technical Architecture

*Architecture diagram coming soon*

### Document Processing Pipeline

**Ingestion:** Contracts are processed with legal-aware chunking that respects clause boundaries and keeps defined terms with their definitions.

**Embedding:** Chunks are embedded with metadata (document type, effective date, section hierarchy) to enable filtered retrieval.

**Indexing:** Dual indexes support hybrid search—vector for semantic similarity, keyword for exact term matching.

### Query Pipeline

**Query analysis:** Determine if the query requires exact term matching (citations, defined terms) or semantic search (concept questions).

**Hybrid retrieval:** Weighted combination of vector similarity (60%) and BM25 keyword matching (40%), tuned for legal text.

**Citation verification:** Extract any citations from retrieved context, verify they exist and support the claimed proposition.

**Generation with grounding:** LLM generates response constrained to retrieved context, with explicit source attribution.

**Hallucination check:** Post-generation verification that all claims are supported by retrieved documents.

## Evaluation Methodology

Reliability claims require measurement. Here's how this demo is evaluated:

| Metric | Target | Measurement |
|--------|--------|-------------|
| Faithfulness | >0.95 | RAGAS framework scoring |
| Citation Accuracy | 100% | Manual audit of 50 citations |
| Refusal Rate | >5% | Percentage of queries where system declines |
| Retrieval Precision | >0.9 | Relevant docs in top-5 results |

Weekly evaluation runs against a test set of 100 contract review queries.

## Limitations & Scope

This is a demonstration, not a production system. Key limitations:

- **No Westlaw/Lexis integration.** Uses public contract datasets and sample documents.
- **Limited jurisdiction coverage.** Focused on U.S. commercial contracts.
- **No matter management.** Single-user demo, not multi-tenant with ethical walls.
- **No audit logging.** Production systems need comprehensive logging for compliance.

These limitations are intentional—the goal is demonstrating RAG configuration principles, not building a CoCounsel competitor.

## Repository & Documentation

*GitHub repository coming soon*

The repository will include:
- Full source code with inline documentation
- Configuration rationale for every architectural decision
- Evaluation scripts and test datasets
- Setup instructions for local deployment

## Related Content

- **Strategic context:** [Legal AI Roadmap](/analysis/legal-ai-roadmap)
